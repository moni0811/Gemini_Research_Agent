{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "775aa4f5d2ef46b7840207feca135ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79e40694e14a40b9a78c6b890384e718",
              "IPY_MODEL_5da5fbf758eb4ad38a9e243b1ff07d00",
              "IPY_MODEL_db7f0c39b7ce45c78ab18844b2f0be00"
            ],
            "layout": "IPY_MODEL_ce737718dfe949838f5b71df8f7077e1"
          }
        },
        "79e40694e14a40b9a78c6b890384e718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4286a07de81d4c35ab23642d8a39a343",
            "placeholder": "​",
            "style": "IPY_MODEL_26d794e9379847c4acaf027d27af6e47",
            "value": "Downloading data: 100%"
          }
        },
        "5da5fbf758eb4ad38a9e243b1ff07d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de0a7fc719c4583820de29164c52cfb",
            "max": 253475402,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8af102da4f24249b203e092813b7330",
            "value": 253475402
          }
        },
        "db7f0c39b7ce45c78ab18844b2f0be00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a548a7a38434d3bb4448c03c7f11739",
            "placeholder": "​",
            "style": "IPY_MODEL_f4842e9aeeee4d9ea3d166171d55626c",
            "value": " 253M/253M [00:04&lt;00:00, 54.7MB/s]"
          }
        },
        "ce737718dfe949838f5b71df8f7077e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4286a07de81d4c35ab23642d8a39a343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d794e9379847c4acaf027d27af6e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de0a7fc719c4583820de29164c52cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8af102da4f24249b203e092813b7330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a548a7a38434d3bb4448c03c7f11739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4842e9aeeee4d9ea3d166171d55626c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61d0909ddd5447bc981f97f1ed0292bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e1f6bd486f94813a7038f15ee6f42ac",
              "IPY_MODEL_6c53aea964564ace99b61144172a197a",
              "IPY_MODEL_cdcabf101ecc46be848f50e4a343787b"
            ],
            "layout": "IPY_MODEL_32ef9caa4abe455596ae1660a54c4c67"
          }
        },
        "5e1f6bd486f94813a7038f15ee6f42ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87d96f01e2914c1f9c3f59b5d8f6967f",
            "placeholder": "​",
            "style": "IPY_MODEL_932d7125dfdc437681fcbb40974d0e02",
            "value": "Generating train split: 100%"
          }
        },
        "6c53aea964564ace99b61144172a197a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1877b2f19f84f36a1f7f14c9a3b476b",
            "max": 209760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58b099acbdd448c9b38a560f60f8974d",
            "value": 209760
          }
        },
        "cdcabf101ecc46be848f50e4a343787b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85cf78093ad4366813e65b7cc4595f4",
            "placeholder": "​",
            "style": "IPY_MODEL_f585f5fa5f004f3996882e9f89ac3855",
            "value": " 209760/209760 [00:02&lt;00:00, 108366.97 examples/s]"
          }
        },
        "32ef9caa4abe455596ae1660a54c4c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d96f01e2914c1f9c3f59b5d8f6967f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932d7125dfdc437681fcbb40974d0e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1877b2f19f84f36a1f7f14c9a3b476b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b099acbdd448c9b38a560f60f8974d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b85cf78093ad4366813e65b7cc4595f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f585f5fa5f004f3996882e9f89ac3855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install graphviz libgraphviz-dev pkg-config"
      ],
      "metadata": {
        "id": "GkrOQdq-lgnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "    datasets==2.19.1 \\\n",
        "    langchain-pinecone==0.1.1 \\\n",
        "    langchain-openai==0.1.9 \\\n",
        "    langchain==0.2.5 \\\n",
        "    langchain-core==0.2.9 \\\n",
        "    langgraph==0.1.1 \\\n",
        "    semantic-router==0.0.48 \\\n",
        "    serpapi==0.1.5 \\\n",
        "    google-search-results==2.4.2 \\\n",
        "    pygraphviz==1.12 \\\n",
        "    fsspec # for visualizing"
      ],
      "metadata": {
        "id": "7e4pPaQolnXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai langchain-google-genai pyowm langchain_community"
      ],
      "metadata": {
        "id": "p-Qt4NtqZi3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRL_Daswolsd"
      },
      "outputs": [],
      "source": [
        "pip install numpy==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "GEMINI_API_KEY = getpass.getpass(\"Enter Gemini API Key: \")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "PC_API_KEY = getpass.getpass(\"Enter Pinecone API Key: \")\n",
        "os.environ[\"PC_API_KEY\"] = PC_API_KEY\n",
        "\n",
        "serpapi_key=getpass.getpass(\"Enter SerpAPI key: \")\n",
        "\n",
        "OPENWEATHERMAP_API_KEY = getpass.getpass(\"Enter OpenWeatherMap API Key: \")\n",
        "os.environ[\"OPENWEATHERMAP_API_KEY\"]=OPENWEATHERMAP_API_KEY"
      ],
      "metadata": {
        "id": "89AdSSmWmCas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7dc8e02-7c2e-402f-e070-8ef166056d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Gemini API Key: ··········\n",
            "Enter Pinecone API Key: ··········\n",
            "Enter SerpAPI key: ··········\n",
            "Enter OpenWeatherMap API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train\")\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# Use us-central1 to match the quota region from the error\n",
        "encoder = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "    quota_location=\"us-central1\"  # Changed from asia-east1\n",
        ")\n",
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=PC_API_KEY)\n",
        "\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"  # us-east-1, us-west-2\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "index_name=\"gemini-1-5-flash-research-agent\"\n",
        "\n",
        "# Get embedding dimension\n",
        "dims = len(encoder.embed_query(\"test\"))\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dims,\n",
        "        metric=\"dotproduct\",\n",
        "        spec=spec,\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "      time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "\n",
        "# Check if index already has data\n",
        "stats = index.describe_index_stats()\n",
        "print(f\"Index stats: {stats}\")\n",
        "\n",
        "# Skip embedding if index already has data (for testing)\n",
        "if stats['total_vector_count'] > 0:\n",
        "    print(f\"Index already contains {stats['total_vector_count']} vectors. Skipping embedding process.\")\n",
        "    skip_embedding = True\n",
        "else:\n",
        "    skip_embedding = False\n",
        "\n",
        "# Only run embedding process if index is empty\n",
        "if not skip_embedding:\n",
        "    from tqdm.auto import tqdm\n",
        "    import time\n",
        "\n",
        "    # For faster testing, you can use a smaller dataset\n",
        "    data=dataset.to_pandas().iloc[:100]\n",
        "\n",
        "    # Reduce batch size to avoid rate limits\n",
        "    batch_size=32  # Reduced from 128\n",
        "\n",
        "    print(f\"Processing {len(data)} documents in batches of {batch_size}\")\n",
        "    print(\"This will take some time due to rate limiting...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        i_end = min(len(data), i+batch_size)\n",
        "        batch = data[i:i_end].to_dict(orient=\"records\")\n",
        "        metadata=[{\"title\":r[\"title\"],\n",
        "                   \"content\":r[\"content\"],\n",
        "                   \"arxiv_id\":r[\"arxiv_id\"],\n",
        "                   \"references\":r[\"references\"].tolist()\n",
        "                   } for r in batch]\n",
        "        ids=[r[\"id\"] for r in batch]\n",
        "\n",
        "        # Process embeddings with rate limiting\n",
        "        embeds = []\n",
        "        for j, record in enumerate(batch):\n",
        "            try:\n",
        "                embed = encoder.embed_query(record[\"content\"])\n",
        "                embeds.append(embed)\n",
        "\n",
        "                # Add small delay between individual embedding calls\n",
        "                if j < len(batch) - 1:  # Don't delay after the last item\n",
        "                    time.sleep(0.1)  # 100ms delay between calls\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding content for ID {record['id']}: {e}\")\n",
        "                # Use a zero vector as fallback\n",
        "                embeds.append([0.0] * dims)\n",
        "\n",
        "        # Upsert to Pinecone\n",
        "        try:\n",
        "            index.upsert(vectors=zip(ids, embeds, metadata))\n",
        "        except Exception as e:\n",
        "            print(f\"Error upserting batch {i//batch_size + 1}: {e}\")\n",
        "\n",
        "        # Add delay between batches to respect rate limits\n",
        "        # With 32 items per batch and 150 requests/minute limit, we need ~13 second delays\n",
        "        if i + batch_size < len(data):  # Don't delay after the last batch\n",
        "            print(f\"Completed batch {i//batch_size + 1}, waiting 15 seconds...\")\n",
        "            time.sleep(15)  # 15 second delay between batches\n",
        "else:\n",
        "    print(\"Using existing embeddings in the index.\")\n",
        "\n",
        "from typing import TypedDict, Annotated, List, Union\n",
        "from langchain_core.agents import AgentAction, AgentFinish\n",
        "from langchain_core.messages import BaseMessage\n",
        "import operator\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    input: str\n",
        "    chat_history: list[BaseMessage]\n",
        "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
        "\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "serpapi_params = {\n",
        "    \"engine\": \"google\",\n",
        "    \"api_key\": serpapi_key\n",
        "}\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "abstract_pattern = re.compile(\n",
        "    r\"<blockquote class=\\\"abstract mathjax\\\">\\n[ ]*<span class=\\\"abstract-title\\\">Abstract:</span>(.*?)<\\/blockquote>\",\n",
        "    re.DOTALL,\n",
        ")\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool(\"fetch_arxiv\")\n",
        "def fetch_arxiv(arxiv_id: str) -> str:\n",
        "    \"\"\"Gets the abstract from an ArXiv paper given the arxiv ID. Useful for\n",
        "    finding high-level context about a specific paper.\"\"\"\n",
        "\n",
        "    # get paper page in html\n",
        "    res = requests.get(\n",
        "        f\"https://export.arxiv.org/abs/{arxiv_id}\"\n",
        "    )\n",
        "    # search html for abstract\n",
        "    re_match = abstract_pattern.search(res.text)\n",
        "    # return abstract text\n",
        "    return re_match.group(1).strip()\n",
        "\n",
        "@tool(\"web_search\")\n",
        "def web_search(query: str):\n",
        "    \"\"\"Finds general knowledge information using Google search. Can also be used\n",
        "    to augment more 'general' knowledge to a previous specialist query.\"\"\"\n",
        "    search = GoogleSearch({\n",
        "        **serpapi_params,\n",
        "        \"q\": query,\n",
        "        \"num\": 5\n",
        "    })\n",
        "    results = search.get_dict()[\"organic_results\"]\n",
        "    contexts = \"\\n---\\n\".join(\n",
        "        [\"\\n\".join([x[\"title\"], x[\"snippet\"], x[\"link\"]]) for x in results]\n",
        "    )\n",
        "    return contexts\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
        "\n",
        "@tool(\"weather_agent\")\n",
        "def weather_agent(location: str) -> str:\n",
        "    \"\"\"Fetches weather information for a given location.\"\"\"\n",
        "    context = OpenWeatherMapQueryRun(api_key=os.environ[\"OPENWEATHERMAP_API_KEY\"])\n",
        "    return f\"The current weather in {location} is {context.run(location)}\"\n",
        "\n",
        "def format_rag_contexts(matches: list):\n",
        "    contexts = []\n",
        "    for x in matches:\n",
        "        text = (\n",
        "            f\"Title: {x['metadata']['title']}\\n\"\n",
        "            f\"Content: {x['metadata']['content']}\\n\"\n",
        "            f\"ArXiv ID: {x['metadata']['arxiv_id']}\\n\"\n",
        "            f\"Related Papers: {x['metadata']['references']}\\n\"\n",
        "        )\n",
        "        contexts.append(text)\n",
        "    context_str = \"\\n---\\n\".join(contexts)\n",
        "    return context_str\n",
        "\n",
        "@tool(\"rag_search_filter\")\n",
        "def rag_search_filter(query: str, arxiv_id: str) -> str:\n",
        "    \"\"\"Finds information from our ArXiv database using a natural language query\n",
        "    and a specific ArXiv ID. Allows us to learn more details about a specific paper.\"\"\"\n",
        "    xq = encoder.embed_query(query)\n",
        "    xc = index.query(vector=xq, top_k=6, include_metadata=True, filter={\"arxiv_id\": arxiv_id})\n",
        "    context_str = format_rag_contexts(xc[\"matches\"])\n",
        "    return context_str\n",
        "\n",
        "@tool(\"rag_search\")\n",
        "def rag_search(query: str) -> str:\n",
        "    \"\"\"Finds specialist information on AI using a natural language query.\"\"\"\n",
        "    xq = encoder.embed_query(query)\n",
        "    xc = index.query(vector=xq, top_k=2, include_metadata=True)\n",
        "    context_str = format_rag_contexts(xc[\"matches\"])\n",
        "    return context_str\n",
        "\n",
        "@tool(\"final_answer\")\n",
        "def final_answer(\n",
        "    introduction: str,\n",
        "    research_steps: str,\n",
        "    main_body: str,\n",
        "    conclusion: str,\n",
        "    sources: str\n",
        "):\n",
        "    \"\"\"Returns a natural language response to the user in the form of a research\n",
        "    report. There are several sections to this report, those are:\n",
        "    - `introduction`: a short paragraph introducing the user's question and the\n",
        "    topic we are researching.\n",
        "    - `research_steps`: a few bullet points explaining the steps that were taken\n",
        "    to research your report.\n",
        "    - `main_body`: this is where the bulk of high quality and concise\n",
        "    information that answers the user's question belongs. It is 3-4 paragraphs\n",
        "    long in length.\n",
        "    - `conclusion`: this is a short single paragraph conclusion providing a\n",
        "    concise but sophisticated view on what was found.\n",
        "    - `sources`: a bulletpoint list provided detailed sources for all information\n",
        "    referenced during the research process\n",
        "    \"\"\"\n",
        "    if type(research_steps) is list:\n",
        "        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n",
        "    if type(sources) is list:\n",
        "        sources = \"\\n\".join([f\"- {s}\" for s in sources])\n",
        "    return \"\"\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Updated system prompt for better control\n",
        "system_prompt = \"\"\"You are the oracle, the great AI decision maker.\n",
        "Given the user's query you must decide what to do with it based on the\n",
        "list of tools provided to you.\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. NEVER use the same tool more than ONCE with the same or similar input\n",
        "2. For queries about \"news and weather\":\n",
        "   - Use web_search ONCE for news information\n",
        "   - Use weather_agent ONCE for weather information\n",
        "   - Then use final_answer to provide the response\n",
        "3. If you see a tool has been used in the scratchpad, DO NOT use it again\n",
        "4. Always progress toward final_answer - do not get stuck in loops\n",
        "5. If you have gathered enough information, use final_answer immediately\n",
        "\n",
        "Current available tools will be filtered to prevent overuse. Choose the most appropriate tool that hasn't been used yet.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"assistant\", \"scratchpad: {scratchpad}\"),\n",
        "])\n",
        "\n",
        "from langchain_core.messages import ToolCall, ToolMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0 # Changed from asia-east1 for consistency\n",
        ")\n",
        "\n",
        "# Define tools based on query type\n",
        "def get_tools_for_query(query: str):\n",
        "    \"\"\"Return appropriate tools based on the query\"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # For news and weather queries, only return those tools\n",
        "    if any(word in query_lower for word in [\"news\", \"weather\"]) and \\\n",
        "       (\"news\" in query_lower and \"weather\" in query_lower):\n",
        "        return [web_search, weather_agent, final_answer]\n",
        "\n",
        "    # For other queries, return all tools\n",
        "    return [rag_search_filter, rag_search, fetch_arxiv, web_search, weather_agent, final_answer]\n",
        "\n",
        "# define a function to transform intermediate_steps from list\n",
        "# of AgentAction to scratchpad string\n",
        "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
        "    research_steps = []\n",
        "    for i, action in enumerate(intermediate_steps):\n",
        "        if action.log != \"TBD\":\n",
        "            # this was the ToolExecution\n",
        "            research_steps.append(\n",
        "                f\"Tool: {action.tool}, input: {action.tool_input}\\n\"\n",
        "                f\"Output: {action.log}\"\n",
        "            )\n",
        "    return \"\\n---\\n\".join(research_steps)\n",
        "\n",
        "def run_oracle(state: dict):\n",
        "    print(\"run_oracle\")\n",
        "    print(f\"intermediate_steps: {len(state['intermediate_steps'])}\")\n",
        "\n",
        "    # Hard limit to prevent infinite recursion\n",
        "    if len(state['intermediate_steps']) >= 5:\n",
        "        print(\"Maximum steps reached, forcing final_answer\")\n",
        "        action_out = AgentAction(\n",
        "            tool=\"final_answer\",\n",
        "            tool_input={\n",
        "                \"introduction\": \"Here's the available information based on the research conducted.\",\n",
        "                \"research_steps\": \"Multiple research steps were taken to gather information.\",\n",
        "                \"main_body\": \"\\n\\n\".join([step.log for step in state[\"intermediate_steps\"] if step.log != \"TBD\"]),\n",
        "                \"conclusion\": \"This concludes the research based on available data.\",\n",
        "                \"sources\": \"Various tools and APIs were used to gather this information.\"\n",
        "            },\n",
        "            log=\"TBD\"\n",
        "        )\n",
        "        return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "    # Track used tools\n",
        "    used_tools = [step.tool for step in state[\"intermediate_steps\"]]\n",
        "    tool_counts = {}\n",
        "    for tool in used_tools:\n",
        "        tool_counts[tool] = tool_counts.get(tool, 0) + 1\n",
        "\n",
        "    print(f\"Used tools: {used_tools}\")\n",
        "    print(f\"Tool counts: {tool_counts}\")\n",
        "\n",
        "    # Prevent any tool from being used more than twice\n",
        "    for tool, count in tool_counts.items():\n",
        "        if count >= 2:\n",
        "            print(f\"Tool {tool} used {count} times, limiting further use\")\n",
        "\n",
        "    # Check if this is a news and weather query\n",
        "    query_lower = state[\"input\"].lower()\n",
        "    is_news_weather_query = (\"news\" in query_lower and \"weather\" in query_lower)\n",
        "\n",
        "    if is_news_weather_query:\n",
        "        # Force progression: if weather_agent used but no web_search, force web_search\n",
        "        if \"weather_agent\" in used_tools and \"web_search\" not in used_tools:\n",
        "            print(\"Weather tool used, forcing web_search for news\")\n",
        "            action_out = AgentAction(\n",
        "                tool=\"web_search\",\n",
        "                tool_input={\"query\": \"latest news Chennai today\"},\n",
        "                log=\"TBD\"\n",
        "            )\n",
        "            return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "        # If both tools used, go to final answer\n",
        "        if \"weather_agent\" in used_tools and \"web_search\" in used_tools:\n",
        "            print(\"Both news and weather tools used, forcing final_answer\")\n",
        "            action_out = AgentAction(\n",
        "                tool=\"final_answer\",\n",
        "                tool_input={\n",
        "                    \"introduction\": \"Here's the requested news and weather information for Chennai.\",\n",
        "                    \"research_steps\": \"Used web search for news and weather agent for weather data.\",\n",
        "                    \"main_body\": \"\\n\\n\".join([step.log for step in state[\"intermediate_steps\"] if step.log != \"TBD\"]),\n",
        "                    \"conclusion\": \"This concludes the news and weather update for Chennai.\",\n",
        "                    \"sources\": \"Web search results and OpenWeatherMap API\"\n",
        "                },\n",
        "                log=\"TBD\"\n",
        "            )\n",
        "            return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "    # Get appropriate tools for this query\n",
        "    available_tools = get_tools_for_query(state[\"input\"])\n",
        "\n",
        "    # Filter out tools that have been used too many times\n",
        "    filtered_tools = []\n",
        "    for tool in available_tools:\n",
        "        if tool_counts.get(tool.name, 0) < 2:\n",
        "            filtered_tools.append(tool)\n",
        "\n",
        "    # If no tools available, force final_answer\n",
        "    if not filtered_tools or len(filtered_tools) == 1 and filtered_tools[0].name == \"final_answer\":\n",
        "        print(\"No more tools available, forcing final_answer\")\n",
        "        action_out = AgentAction(\n",
        "            tool=\"final_answer\",\n",
        "            tool_input={\n",
        "                \"introduction\": \"Here's the information gathered so far.\",\n",
        "                \"research_steps\": \"Research was conducted using available tools.\",\n",
        "                \"main_body\": \"\\n\\n\".join([step.log for step in state[\"intermediate_steps\"] if step.log != \"TBD\"]),\n",
        "                \"conclusion\": \"This concludes the available information.\",\n",
        "                \"sources\": \"Multiple sources were consulted.\"\n",
        "            },\n",
        "            log=\"TBD\"\n",
        "        )\n",
        "        return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "    print(f\"Available tools after filtering: {[t.name for t in filtered_tools]}\")\n",
        "\n",
        "    # Create oracle with filtered tools\n",
        "    oracle = (\n",
        "        {\n",
        "            \"input\": lambda x: x[\"input\"],\n",
        "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "            \"scratchpad\": lambda x: create_scratchpad(\n",
        "                intermediate_steps=x[\"intermediate_steps\"]\n",
        "            ),\n",
        "        }\n",
        "        | prompt\n",
        "        | llm.bind_tools(filtered_tools, tool_choice=\"any\")\n",
        "    )\n",
        "\n",
        "    out = oracle.invoke(state)\n",
        "\n",
        "    if not out.tool_calls:\n",
        "        print(\"No tool calls returned, forcing final_answer\")\n",
        "        action_out = AgentAction(\n",
        "            tool=\"final_answer\",\n",
        "            tool_input={\n",
        "                \"introduction\": \"Unable to process the query properly.\",\n",
        "                \"research_steps\": \"No specific research steps were taken.\",\n",
        "                \"main_body\": \"The system encountered an issue processing your request.\",\n",
        "                \"conclusion\": \"Please try rephrasing your query.\",\n",
        "                \"sources\": \"None\"\n",
        "            },\n",
        "            log=\"TBD\"\n",
        "        )\n",
        "        return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "    tool_name = out.tool_calls[0][\"name\"]\n",
        "    tool_args = out.tool_calls[0][\"args\"]\n",
        "\n",
        "    # Additional check: if tool was used too many times, force final_answer\n",
        "    if tool_counts.get(tool_name, 0) >= 2:\n",
        "        print(f\"Tool {tool_name} already used too many times, forcing final_answer\")\n",
        "        action_out = AgentAction(\n",
        "            tool=\"final_answer\",\n",
        "            tool_input={\n",
        "                \"introduction\": \"Research complete based on available information.\",\n",
        "                \"research_steps\": \"Multiple tools were used to gather information.\",\n",
        "                \"main_body\": \"\\n\\n\".join([step.log for step in state[\"intermediate_steps\"] if step.log != \"TBD\"]),\n",
        "                \"conclusion\": \"This concludes the research.\",\n",
        "                \"sources\": \"Various sources were consulted.\"\n",
        "            },\n",
        "            log=\"TBD\"\n",
        "        )\n",
        "        return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "    action_out = AgentAction(\n",
        "        tool=tool_name,\n",
        "        tool_input=tool_args,\n",
        "        log=\"TBD\"\n",
        "    )\n",
        "\n",
        "    return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "def router(state: dict):\n",
        "    # return the tool name to use\n",
        "    if isinstance(state[\"intermediate_steps\"], list) and state[\"intermediate_steps\"]:\n",
        "        tool_name = state[\"intermediate_steps\"][-1].tool\n",
        "        print(f\"Router directing to: {tool_name}\")\n",
        "        return tool_name\n",
        "    else:\n",
        "        # if we output bad format go to final answer\n",
        "        print(\"Router invalid format, going to final_answer\")\n",
        "        return \"final_answer\"\n",
        "\n",
        "tool_str_to_func = {\n",
        "    \"rag_search_filter\": rag_search_filter,\n",
        "    \"rag_search\": rag_search,\n",
        "    \"fetch_arxiv\": fetch_arxiv,\n",
        "    \"web_search\": web_search,\n",
        "    \"weather_agent\": weather_agent,\n",
        "    \"final_answer\": final_answer\n",
        "}\n",
        "\n",
        "def run_tool(state: dict):\n",
        "    # use this as helper function so we repeat less code\n",
        "    tool_name = state[\"intermediate_steps\"][-1].tool\n",
        "    tool_args = state[\"intermediate_steps\"][-1].tool_input\n",
        "    print(f\"{tool_name}.invoke(input={tool_args})\")\n",
        "\n",
        "    # run tool\n",
        "    try:\n",
        "        out = tool_str_to_func[tool_name].invoke(input=tool_args)\n",
        "        action_out = AgentAction(\n",
        "            tool=tool_name,\n",
        "            tool_input=tool_args,\n",
        "            log=str(out)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error running tool {tool_name}: {e}\")\n",
        "        action_out = AgentAction(\n",
        "            tool=tool_name,\n",
        "            tool_input=tool_args,\n",
        "            log=f\"Error: {str(e)}\"\n",
        "        )\n",
        "\n",
        "    return {\"intermediate_steps\": [action_out]}\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"oracle\", run_oracle)\n",
        "graph.add_node(\"rag_search_filter\", run_tool)\n",
        "graph.add_node(\"rag_search\", run_tool)\n",
        "graph.add_node(\"fetch_arxiv\", run_tool)\n",
        "graph.add_node(\"web_search\", run_tool)\n",
        "graph.add_node(\"final_answer\", run_tool)\n",
        "graph.add_node(\"weather_agent\", run_tool)\n",
        "\n",
        "graph.set_entry_point(\"oracle\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    source=\"oracle\",  # where in graph to start\n",
        "    path=router,  # function to determine which node is called\n",
        ")\n",
        "\n",
        "# create edges from each tool back to the oracle\n",
        "for tool_name in tool_str_to_func.keys():\n",
        "    if tool_name != \"final_answer\":\n",
        "        graph.add_edge(tool_name, \"oracle\")\n",
        "\n",
        "graph.add_edge(\"final_answer\", END)\n",
        "\n",
        "runnable = graph.compile()\n",
        "\n",
        "def build_report(output: dict):\n",
        "    research_steps = output[\"research_steps\"]\n",
        "    if type(research_steps) is list:\n",
        "        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n",
        "    sources = output[\"sources\"]\n",
        "    if type(sources) is list:\n",
        "        sources = \"\\n\".join([f\"- {s}\" for s in sources])\n",
        "    return f\"\"\"\n",
        "INTRODUCTION\n",
        "------------\n",
        "{output[\"introduction\"]}\n",
        "\n",
        "RESEARCH STEPS\n",
        "--------------\n",
        "{research_steps}\n",
        "\n",
        "REPORT\n",
        "------\n",
        "{output[\"main_body\"]}\n",
        "\n",
        "CONCLUSION\n",
        "----------\n",
        "{output[\"conclusion\"]}\n",
        "\n",
        "SOURCES\n",
        "-------\n",
        "{sources}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zlNtFu2TmGZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "775aa4f5d2ef46b7840207feca135ba3",
            "79e40694e14a40b9a78c6b890384e718",
            "5da5fbf758eb4ad38a9e243b1ff07d00",
            "db7f0c39b7ce45c78ab18844b2f0be00",
            "ce737718dfe949838f5b71df8f7077e1",
            "4286a07de81d4c35ab23642d8a39a343",
            "26d794e9379847c4acaf027d27af6e47",
            "5de0a7fc719c4583820de29164c52cfb",
            "f8af102da4f24249b203e092813b7330",
            "5a548a7a38434d3bb4448c03c7f11739",
            "f4842e9aeeee4d9ea3d166171d55626c",
            "61d0909ddd5447bc981f97f1ed0292bb",
            "5e1f6bd486f94813a7038f15ee6f42ac",
            "6c53aea964564ace99b61144172a197a",
            "cdcabf101ecc46be848f50e4a343787b",
            "32ef9caa4abe455596ae1660a54c4c67",
            "87d96f01e2914c1f9c3f59b5d8f6967f",
            "932d7125dfdc437681fcbb40974d0e02",
            "b1877b2f19f84f36a1f7f14c9a3b476b",
            "58b099acbdd448c9b38a560f60f8974d",
            "b85cf78093ad4366813e65b7cc4595f4",
            "f585f5fa5f004f3996882e9f89ac3855"
          ]
        },
        "outputId": "317bb57a-0474-4c34-d353-94cc28b24ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/253M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "775aa4f5d2ef46b7840207feca135ba3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/209760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61d0909ddd5447bc981f97f1ed0292bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index stats: {'dimension': 768,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'': {'vector_count': 656}},\n",
            " 'total_vector_count': 656}\n",
            "Index already contains 656 vectors. Skipping embedding process.\n",
            "Using existing embeddings in the index.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langgraph/graph/graph.py:31: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langgraph.pregel import Channel, Pregel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = runnable.invoke({\n",
        "    \"input\": \"send me news and weather in Chennai\",\n",
        "    \"chat_history\": [],\n",
        "})\n",
        "\n",
        "print(build_report(\n",
        "    output=out[\"intermediate_steps\"][-1].tool_input\n",
        "))"
      ],
      "metadata": {
        "id": "YWR49L2umO6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3d14e4-bcdd-40a2-cd45-3af5c03fdebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_oracle\n",
            "intermediate_steps: 0\n",
            "Used tools: []\n",
            "Tool counts: {}\n",
            "Available tools after filtering: ['web_search', 'weather_agent', 'final_answer']\n",
            "Router directing to: weather_agent\n",
            "weather_agent.invoke(input={'location': 'Chennai'})\n",
            "run_oracle\n",
            "intermediate_steps: 2\n",
            "Used tools: ['weather_agent', 'weather_agent']\n",
            "Tool counts: {'weather_agent': 2}\n",
            "Tool weather_agent used 2 times, limiting further use\n",
            "Weather tool used, forcing web_search for news\n",
            "Router directing to: web_search\n",
            "web_search.invoke(input={'query': 'latest news Chennai today'})\n",
            "run_oracle\n",
            "intermediate_steps: 4\n",
            "Used tools: ['weather_agent', 'weather_agent', 'web_search', 'web_search']\n",
            "Tool counts: {'weather_agent': 2, 'web_search': 2}\n",
            "Tool weather_agent used 2 times, limiting further use\n",
            "Tool web_search used 2 times, limiting further use\n",
            "Both news and weather tools used, forcing final_answer\n",
            "Router directing to: final_answer\n",
            "final_answer.invoke(input={'introduction': \"Here's the requested news and weather information for Chennai.\", 'research_steps': 'Used web search for news and weather agent for weather data.', 'main_body': \"The current weather in Chennai is In Chennai, the current weather is as follows:\\nDetailed status: thunderstorm\\nWind speed: 5.14 m/s, direction: 270°\\nHumidity: 85%\\nTemperature: \\n  - Current: 28.09°C\\n  - High: 28.31°C\\n  - Low: 26.68°C\\n  - Feels like: 33.15°C\\nRain: {}\\nHeat index: None\\nCloud cover: 40%\\n\\nChennai News Today\\nTrenches filled with sewage water chokes Indira Gandhi Street in Arumbakkam. Damaged pipelines, dug-up roads leave Arumbakkam residents struggling · K. Lakshmi.\\nhttps://www.thehindu.com/news/cities/chennai/\\n---\\nLatest Chennai News Headlines & Coronavirus ...\\nCheck out the latest news in Chennai on The Times of India with a wide range of topics including Chennai's politics, Chennai crime, sports, fashion ...\\nhttps://timesofindia.indiatimes.com/city/chennai\\n---\\nLatest Chennai News, Chennai News Today and Headlines\\nChennai News Today: Find Chennai latest news and headlines on weather updates, entertainment politics, schools, crime, festivals and real estate.\\nhttps://indianexpress.com/section/cities/chennai/\\n---\\nLatest and Breaking Chennai News\\n15 Tamil Nadu Fishermen Return From Iran, Efforts On To Rescue Others · 3 Chennai College Students Drown In Coimbatore River · 1 Kg Cocaine Worth Rs 6 Crore ...\\nhttps://www.ndtv.com/chennai-news\", 'conclusion': 'This concludes the news and weather update for Chennai.', 'sources': 'Web search results and OpenWeatherMap API'})\n",
            "\n",
            "INTRODUCTION\n",
            "------------\n",
            "Here's the requested news and weather information for Chennai.\n",
            "\n",
            "RESEARCH STEPS\n",
            "--------------\n",
            "Used web search for news and weather agent for weather data.\n",
            "\n",
            "REPORT\n",
            "------\n",
            "The current weather in Chennai is In Chennai, the current weather is as follows:\n",
            "Detailed status: thunderstorm\n",
            "Wind speed: 5.14 m/s, direction: 270°\n",
            "Humidity: 85%\n",
            "Temperature: \n",
            "  - Current: 28.09°C\n",
            "  - High: 28.31°C\n",
            "  - Low: 26.68°C\n",
            "  - Feels like: 33.15°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 40%\n",
            "\n",
            "Chennai News Today\n",
            "Trenches filled with sewage water chokes Indira Gandhi Street in Arumbakkam. Damaged pipelines, dug-up roads leave Arumbakkam residents struggling · K. Lakshmi.\n",
            "https://www.thehindu.com/news/cities/chennai/\n",
            "---\n",
            "Latest Chennai News Headlines & Coronavirus ...\n",
            "Check out the latest news in Chennai on The Times of India with a wide range of topics including Chennai's politics, Chennai crime, sports, fashion ...\n",
            "https://timesofindia.indiatimes.com/city/chennai\n",
            "---\n",
            "Latest Chennai News, Chennai News Today and Headlines\n",
            "Chennai News Today: Find Chennai latest news and headlines on weather updates, entertainment politics, schools, crime, festivals and real estate.\n",
            "https://indianexpress.com/section/cities/chennai/\n",
            "---\n",
            "Latest and Breaking Chennai News\n",
            "15 Tamil Nadu Fishermen Return From Iran, Efforts On To Rescue Others · 3 Chennai College Students Drown In Coimbatore River · 1 Kg Cocaine Worth Rs 6 Crore ...\n",
            "https://www.ndtv.com/chennai-news\n",
            "\n",
            "CONCLUSION\n",
            "----------\n",
            "This concludes the news and weather update for Chennai.\n",
            "\n",
            "SOURCES\n",
            "-------\n",
            "Web search results and OpenWeatherMap API\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = runnable.invoke({\n",
        "    \"input\": \"tell me about RAG\",\n",
        "    \"chat_history\": [],\n",
        "})\n",
        "\n",
        "print(build_report(\n",
        "    output=out[\"intermediate_steps\"][-1].tool_input\n",
        "))"
      ],
      "metadata": {
        "id": "VfB_zLy9mRZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312592a8-12bd-48a4-dbc9-3223c581db33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_oracle\n",
            "intermediate_steps: 0\n",
            "Used tools: []\n",
            "Tool counts: {}\n",
            "Available tools after filtering: ['rag_search_filter', 'rag_search', 'fetch_arxiv', 'web_search', 'weather_agent', 'final_answer']\n",
            "Router directing to: rag_search\n",
            "rag_search.invoke(input={'query': 'What is RAG?'})\n",
            "run_oracle\n",
            "intermediate_steps: 2\n",
            "Used tools: ['rag_search', 'rag_search']\n",
            "Tool counts: {'rag_search': 2}\n",
            "Tool rag_search used 2 times, limiting further use\n",
            "Available tools after filtering: ['rag_search_filter', 'fetch_arxiv', 'web_search', 'weather_agent', 'final_answer']\n",
            "Router directing to: rag_search_filter\n",
            "rag_search_filter.invoke(input={'arxiv_id': '2311.04072', 'query': 'What is RAG?'})\n",
            "run_oracle\n",
            "intermediate_steps: 4\n",
            "Used tools: ['rag_search', 'rag_search', 'rag_search_filter', 'rag_search_filter']\n",
            "Tool counts: {'rag_search': 2, 'rag_search_filter': 2}\n",
            "Tool rag_search used 2 times, limiting further use\n",
            "Tool rag_search_filter used 2 times, limiting further use\n",
            "Available tools after filtering: ['fetch_arxiv', 'web_search', 'weather_agent', 'final_answer']\n",
            "Router directing to: web_search\n",
            "web_search.invoke(input={'query': 'What is RAG?'})\n",
            "run_oracle\n",
            "intermediate_steps: 6\n",
            "Maximum steps reached, forcing final_answer\n",
            "Router directing to: final_answer\n",
            "final_answer.invoke(input={'introduction': \"Here's the available information based on the research conducted.\", 'research_steps': 'Multiple research steps were taken to gather information.', 'main_body': \"Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: 5 Preprint. Relationship with RL Our method can be considered as a simplified but efficient version of RL. Using typical PPO method (Schulman et al., 2017) as an example, its objective is to optimize the actor model (i.e., the initial model Ï Î¸) to maximize the expected reward score, formally given as: fielder, X) PPO = (Zan â Ay ) ; 3 Â» Tous (GelG<t,X) â where AË yt is the advantage function of the Ë yt token returned by the critic model given the reward score R Ë Y . Ï Î¸old is the model before the previous parameter update. Here, we ignore the clipping function and KL penalty for convenience. Considering the FIGA training objective in Equation 2, our weight functions Ë r(Â·) and Ë r(Â·) in FIGA can be viewed as a simplified advantage function A(Â·) in Equation 3 to evaluate the importance of each token. Therefore, FIGA has a similar objective with RL but with a simplified token-wise reward function. We do not use an extra learned critic model and remove the use of previous rollout model, which makes FIGA more efficient. In the later experiment section, we will verify the effectiveness of our method. # 4 EXPERIMENT 4.1 EXPERIMENTAL SETUP 4.1.1 BASELINE METHODS (1) In order to better evaluate FIGA method, we choose several baselines for comparison: SFT (Ouyang et al., 2022): it continues to fine-tune the initial model using pairs of data with sequence-to-sequence loss. (2) PPO (Ouyang et al., 2022): it optimizes the initial model to achieve a higher reward score provided by the reward model through the PPO algorithm. (3) CoH (Liu et al., 2023a): it annotates the dataset by prefixing â A helpful answer: â and â An unhelpful answer: â\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n---\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: 2 Preprint. ence data (Liu et al., 2023a; Zhang et al., 2023; Dong et al., 2023), the integration of preferences for different outputs into the loss function (Yuan et al., 2023; Rafailov et al., 2023; Zhao et al., 2023b; Liu et al., 2023c), and the utilization of controllable text generation techniques (Lu et al., 2022). However, the human preference information used in these methods is at the sentence level, lacking more fine-grained supervision signals. # 3 APPROACH In this section, we present the proposed alignment approach FIGA by leveraging fine-grained qual- ity signals. Our approach is developed based on a specially curated alignment dataset called SPA (Section 3.1), where each low-quality initial response is paired with a high-quality revised response. Based on such an alignment dataset, we further develop a new loss function that incorporates fine- grained quality signals derived by contrasting good and bad responses (Section 3.2). Our approach is easy to implement (similar to SFT) and can capture the underlying effect to generate high-quality responses instead of simply imitating them (similar to RLHF), which are discussed in Section 3.3. The overall framework of our FIGA pipeline is shown in Figure 1.\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: 5 Preprint. Relationship with RL Our method can be considered as a simplified but efficient version of RL. Using typical PPO method (Schulman et al., 2017) as an example, its objective is to optimize the actor model (i.e., the initial model Ï Î¸) to maximize the expected reward score, formally given as: fielder, X) PPO = (Zan â Ay ) ; 3 Â» Tous (GelG<t,X) â where AË yt is the advantage function of the Ë yt token returned by the critic model given the reward score R Ë Y . Ï Î¸old is the model before the previous parameter update. Here, we ignore the clipping function and KL penalty for convenience. Considering the FIGA training objective in Equation 2, our weight functions Ë r(Â·) and Ë r(Â·) in FIGA can be viewed as a simplified advantage function A(Â·) in Equation 3 to evaluate the importance of each token. Therefore, FIGA has a similar objective with RL but with a simplified token-wise reward function. We do not use an extra learned critic model and remove the use of previous rollout model, which makes FIGA more efficient. In the later experiment section, we will verify the effectiveness of our method. # 4 EXPERIMENT 4.1 EXPERIMENTAL SETUP 4.1.1 BASELINE METHODS (1) In order to better evaluate FIGA method, we choose several baselines for comparison: SFT (Ouyang et al., 2022): it continues to fine-tune the initial model using pairs of data with sequence-to-sequence loss. (2) PPO (Ouyang et al., 2022): it optimizes the initial model to achieve a higher reward score provided by the reward model through the PPO algorithm. (3) CoH (Liu et al., 2023a): it annotates the dataset by prefixing â A helpful answer: â and â An unhelpful answer: â\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n---\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: 2 Preprint. ence data (Liu et al., 2023a; Zhang et al., 2023; Dong et al., 2023), the integration of preferences for different outputs into the loss function (Yuan et al., 2023; Rafailov et al., 2023; Zhao et al., 2023b; Liu et al., 2023c), and the utilization of controllable text generation techniques (Lu et al., 2022). However, the human preference information used in these methods is at the sentence level, lacking more fine-grained supervision signals. # 3 APPROACH In this section, we present the proposed alignment approach FIGA by leveraging fine-grained qual- ity signals. Our approach is developed based on a specially curated alignment dataset called SPA (Section 3.1), where each low-quality initial response is paired with a high-quality revised response. Based on such an alignment dataset, we further develop a new loss function that incorporates fine- grained quality signals derived by contrasting good and bad responses (Section 3.2). Our approach is easy to implement (similar to SFT) and can capture the underlying effect to generate high-quality responses instead of simply imitating them (similar to RLHF), which are discussed in Section 3.3. The overall framework of our FIGA pipeline is shown in Figure 1.\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n---\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: ¥ 0.6 w/o further selection 3.80 3.01 30.2 28.1 27.2 24 56.2 58.5 50.4 57.4 8.1 8 7.4 7.7 Hyper-parameter Î² = 0 Î³ Ì¸= 0 R(Â·) 4.61 4.54 4.54 41.0 41.2 39.7 37.0 32.2 37.8 59.6 60.1 62.9 58.1 56.0 57.1 8.5 8.4 8.2 8.3 8.2 8.2 34.9 32.7 31.4 31.6 29.3 28.1 34.2 33.0 33.4 The results in Table 5 indicate that: (1) Levenshtein distance excels in extracting critical tokens, with over +1.5 average score compared with traditional bag of words method, and over +0.6 above ChatGPT related method. (2) It is necessary to further select the bad tokens returned by Levenshtein distance, as this leads to an average improvement of +6.8. (3) Remaining only the poor-quality to- kens with a negative log-likelihood â ¤ 0.6 is a sensible choice, which aims to penalize tokens that the model is relatively confident in generating, even though their actual quality is subpar. (4) Punishing the undesirable actions is beneficial, as it results in an average increase of +0.7 in comparison to simply encouraging the good actions. (5) Focusing only on good and bad tokens is sufficient, since setting Î³ to a non-zero value leads to a decrease of 1.9 on average. (6) The inferior performance of setting the weights as reward scores can be attributed to intrinsic inaccuracies of the reward scores, especially in out-of-distribution scenarios (Bai et al., 2022b). # 5 CONCLUSION In this paper, we have presented FIGA, a new approach that aligns language models with human preferences, by leveraging fine-grained quality signals to enhance the alignment quality during fine- tuning.\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n---\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: to the responses of corresponding quality, employs SFT on it and computes loss only for the specially masked response tokens. (4) RRHF (Yuan et al., 2023): it applies SFT on the optimal responses, and further optimizes the ranking loss among responses from multiple sources by encouraging the model to achieve a greater conditional log probability for the response that holds a superior ranking. IMPLEMENTATION DETAILS Training Datasets For our SPA dataset mentioned in Section 3.1, we broadly select the follow- ing datasets as our initial instances pool: HH-RLHF (Bai et al., 2022a), ShareGPT (ShareGPT, 2023), Synthetic Instruct GPT-J Pairwise (Dahoas, 2023), Stanford SHP (Ethayarajh et al., 2022), and OpenOrca (Lian et al., 2023). We employ the Alpaca-7b model Taori et al. (2023) as the rollout model for generating responses Ë Y , and gpt-3.5-turbo to revise and obtain Ë Y . The prompt used for revision can be found in Appendix A.2 As for the filtering process, we utilize OpenAssistant/reward-model-deberta-v3-large-v2 (OpenAssistant, 2023) as the reward model. According to the reward score distribution, we empirically set the threshold values Î·1 = 1, Î·2 = 3, Î·3 = 3.5, respectively. The statistics of reward scores and edit operations for the SPA dataset are presented in Table 1, while the distribution of the reward scores is illustrated in Figure 2. We can find that the initial response Ë Y has a large distribution gap with the reference distribution Y , which may cause the model hard to learn from the golden target. In contrast, our revised response is closer to the original distribution but with higher quality, making the rollout model easier to learn. The final SPA dataset we obtained consists of 17,333 instances. Model Details (1) For SFT, we set learning rate to 1e-5 and batch size to 128.\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n---\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: Thus, it canâ t fully capture the correct alignment behaviors even demonstrated by what are good and bad behaviors. In this work, we introduce FIGA, a novel method that aligns language models with human prefer- ences. The core idea is to contrast a low-quality initial response from a LLMâ s output with a cor- responding high-quality revised response by another powerful LLM (e.g., ChatGPT), so that LLMs can be noted with what are newly added (good actions) and what are removed or substituted (bad actions) from such a revision process. Such fine-grained quality signals can be more useful than the widely used response-level quality signal. It can instruct LLMs to emphasize the learning of good actions and penalize the bad actions in a single response. To implement our approach, we first cu- rate an alignment dataset called SPA that pairs an initial response with a revised response under the guidance of the ground-truth demonstrations. We mainly keep the queries that a LLM performs less well on, and perform strict filtering. Further, we design a new fine-tuning method that assigns spe- cific token-level weights to different parts (e.g., good or bad tokens). Our learning loss can directly impose fine-grained reward scores to guide the learning of LLMs for improved alignment. To the best of our knowledge, it is the first attempt that leverages fine-grained quality signals for improving the alignment of LLMs without RL. Our approach can make LLMs better understand what are good and bad behaviors beyond simple imitation. By conducting extensive experiments, we demonstrate that FIGA shows promising performance in aligning language models with human preferences: our approach outperform the initial supervised-finetuned model by notable 3.2 points and the strong PPO method by 1.8 points. # 2 RELATED WORK In this section, we review the related work in the two aspects, namely reinforcement learning from human feedback and alignment without reinforcement learning. Reinforcement learning from human feedback Large-scale pre-training empowers large lan- guage models (LLMs) to acquire extensive knowledge, underscoring their remarkable potential across diverse tasks (Brown et al., 2020; Kojima et al., 2022; Zhang et al., 2022; Chowdhery et al., 2022). Nonetheless, models exclusively focus on next token prediction in pre-training phrase, while do not consider human preferences.\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n---\\nTitle: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\\nContent: 7 Preprint. data. This implies responses of FIGA are more in sync with human preferences, making it an exem- plary alignment model. FIGA also scores the highest on the MMLU benchmark, which demonstrates capable task solving abilities of our method, not just limited to alignment. In summary, FIGAâ s su- perior performance on benchmarks confirms the efficacy of our designing. Moreover, we compare the quality of responses from FIGA and other baselines on the Vicuna and WizardLM benchmarks, specifically evaluating the relative merits of each response. The results of this comparative analysis are illustrated in Figure 3. Mmm FIGAWins * Tie FIGA Loses @mm FIGAWins M Tie FIGA Loses Alpaca 7B 9% Alpaca 7B 12% PPO (SPA) 9% PPO (SPA) 14% PPO (85K) 10% PPO (85K) 13% RRHF 8% RRHF 18% CoH 22% CoH 22% SFT 20% SFT 25% ChatGPT Ek 24% ChatGPT EU 33% 0% 25% 50% 75% 100% 0% 25% 50% 75% 100% Figure 3: Win rate of FIGA vs other baselines on Vicuna (left) and WizardLM (right). 4.3 FURTHER ANALYSIS 4.3.1 PERFORMANCE COMPARISON W.R.T. SUBPAR ALIGNMENT DATASET As mentioned in Section 3.1, the steps involved in constructing the SPA dataset includes: (1) collect existing datasets, encompassing the preference datasets and the typical SFT datasets, (2) filter the data based on reward scores, (3) revise the initial responses using LLM. To examine the effectiveness of each of them, we develop the following dataset variants on which to conduct our FIGA: Preference: we only use preference data to construct initial instances pool D, with 3,971 samples. â ¢ Instruction: we construct the initial instances pool D with typical SFT data that the reward model had not encountered during its training, also totaling 3,971 instances. w/o reward filtering: this variant excludes data filtering based on reward scores. â\\nArXiv ID: 2311.04072\\nRelated Papers: ['2309.00267']\\n\\n\\nWhat is RAG (Retrieval-Augmented Generation)?\\nRAG is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources ...\\nhttps://aws.amazon.com/what-is/retrieval-augmented-generation/\\n---\\nWhat is Retrieval-Augmented Generation (RAG)?\\nRAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases)\\nhttps://cloud.google.com/use-cases/retrieval-augmented-generation\\n---\\nRetrieval-augmented generation\\nRetrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information.\\nhttps://en.wikipedia.org/wiki/Retrieval-augmented_generation\\n---\\nWhat Is Retrieval-Augmented Generation aka RAG\\nRetrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched ...\\nhttps://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\", 'conclusion': 'This concludes the research based on available data.', 'sources': 'Various tools and APIs were used to gather this information.'})\n",
            "\n",
            "INTRODUCTION\n",
            "------------\n",
            "Here's the available information based on the research conducted.\n",
            "\n",
            "RESEARCH STEPS\n",
            "--------------\n",
            "Multiple research steps were taken to gather information.\n",
            "\n",
            "REPORT\n",
            "------\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: 5 Preprint. Relationship with RL Our method can be considered as a simplified but efficient version of RL. Using typical PPO method (Schulman et al., 2017) as an example, its objective is to optimize the actor model (i.e., the initial model Ï Î¸) to maximize the expected reward score, formally given as: fielder, X) PPO = (Zan â Ay ) ; 3 Â» Tous (GelG<t,X) â where AË yt is the advantage function of the Ë yt token returned by the critic model given the reward score R Ë Y . Ï Î¸old is the model before the previous parameter update. Here, we ignore the clipping function and KL penalty for convenience. Considering the FIGA training objective in Equation 2, our weight functions Ë r(Â·) and Ë r(Â·) in FIGA can be viewed as a simplified advantage function A(Â·) in Equation 3 to evaluate the importance of each token. Therefore, FIGA has a similar objective with RL but with a simplified token-wise reward function. We do not use an extra learned critic model and remove the use of previous rollout model, which makes FIGA more efficient. In the later experiment section, we will verify the effectiveness of our method. # 4 EXPERIMENT 4.1 EXPERIMENTAL SETUP 4.1.1 BASELINE METHODS (1) In order to better evaluate FIGA method, we choose several baselines for comparison: SFT (Ouyang et al., 2022): it continues to fine-tune the initial model using pairs of data with sequence-to-sequence loss. (2) PPO (Ouyang et al., 2022): it optimizes the initial model to achieve a higher reward score provided by the reward model through the PPO algorithm. (3) CoH (Liu et al., 2023a): it annotates the dataset by prefixing â A helpful answer: â and â An unhelpful answer: â\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "---\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: 2 Preprint. ence data (Liu et al., 2023a; Zhang et al., 2023; Dong et al., 2023), the integration of preferences for different outputs into the loss function (Yuan et al., 2023; Rafailov et al., 2023; Zhao et al., 2023b; Liu et al., 2023c), and the utilization of controllable text generation techniques (Lu et al., 2022). However, the human preference information used in these methods is at the sentence level, lacking more fine-grained supervision signals. # 3 APPROACH In this section, we present the proposed alignment approach FIGA by leveraging fine-grained qual- ity signals. Our approach is developed based on a specially curated alignment dataset called SPA (Section 3.1), where each low-quality initial response is paired with a high-quality revised response. Based on such an alignment dataset, we further develop a new loss function that incorporates fine- grained quality signals derived by contrasting good and bad responses (Section 3.2). Our approach is easy to implement (similar to SFT) and can capture the underlying effect to generate high-quality responses instead of simply imitating them (similar to RLHF), which are discussed in Section 3.3. The overall framework of our FIGA pipeline is shown in Figure 1.\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: 5 Preprint. Relationship with RL Our method can be considered as a simplified but efficient version of RL. Using typical PPO method (Schulman et al., 2017) as an example, its objective is to optimize the actor model (i.e., the initial model Ï Î¸) to maximize the expected reward score, formally given as: fielder, X) PPO = (Zan â Ay ) ; 3 Â» Tous (GelG<t,X) â where AË yt is the advantage function of the Ë yt token returned by the critic model given the reward score R Ë Y . Ï Î¸old is the model before the previous parameter update. Here, we ignore the clipping function and KL penalty for convenience. Considering the FIGA training objective in Equation 2, our weight functions Ë r(Â·) and Ë r(Â·) in FIGA can be viewed as a simplified advantage function A(Â·) in Equation 3 to evaluate the importance of each token. Therefore, FIGA has a similar objective with RL but with a simplified token-wise reward function. We do not use an extra learned critic model and remove the use of previous rollout model, which makes FIGA more efficient. In the later experiment section, we will verify the effectiveness of our method. # 4 EXPERIMENT 4.1 EXPERIMENTAL SETUP 4.1.1 BASELINE METHODS (1) In order to better evaluate FIGA method, we choose several baselines for comparison: SFT (Ouyang et al., 2022): it continues to fine-tune the initial model using pairs of data with sequence-to-sequence loss. (2) PPO (Ouyang et al., 2022): it optimizes the initial model to achieve a higher reward score provided by the reward model through the PPO algorithm. (3) CoH (Liu et al., 2023a): it annotates the dataset by prefixing â A helpful answer: â and â An unhelpful answer: â\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "---\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: 2 Preprint. ence data (Liu et al., 2023a; Zhang et al., 2023; Dong et al., 2023), the integration of preferences for different outputs into the loss function (Yuan et al., 2023; Rafailov et al., 2023; Zhao et al., 2023b; Liu et al., 2023c), and the utilization of controllable text generation techniques (Lu et al., 2022). However, the human preference information used in these methods is at the sentence level, lacking more fine-grained supervision signals. # 3 APPROACH In this section, we present the proposed alignment approach FIGA by leveraging fine-grained qual- ity signals. Our approach is developed based on a specially curated alignment dataset called SPA (Section 3.1), where each low-quality initial response is paired with a high-quality revised response. Based on such an alignment dataset, we further develop a new loss function that incorporates fine- grained quality signals derived by contrasting good and bad responses (Section 3.2). Our approach is easy to implement (similar to SFT) and can capture the underlying effect to generate high-quality responses instead of simply imitating them (similar to RLHF), which are discussed in Section 3.3. The overall framework of our FIGA pipeline is shown in Figure 1.\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "---\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: ¥ 0.6 w/o further selection 3.80 3.01 30.2 28.1 27.2 24 56.2 58.5 50.4 57.4 8.1 8 7.4 7.7 Hyper-parameter Î² = 0 Î³ Ì¸= 0 R(Â·) 4.61 4.54 4.54 41.0 41.2 39.7 37.0 32.2 37.8 59.6 60.1 62.9 58.1 56.0 57.1 8.5 8.4 8.2 8.3 8.2 8.2 34.9 32.7 31.4 31.6 29.3 28.1 34.2 33.0 33.4 The results in Table 5 indicate that: (1) Levenshtein distance excels in extracting critical tokens, with over +1.5 average score compared with traditional bag of words method, and over +0.6 above ChatGPT related method. (2) It is necessary to further select the bad tokens returned by Levenshtein distance, as this leads to an average improvement of +6.8. (3) Remaining only the poor-quality to- kens with a negative log-likelihood â ¤ 0.6 is a sensible choice, which aims to penalize tokens that the model is relatively confident in generating, even though their actual quality is subpar. (4) Punishing the undesirable actions is beneficial, as it results in an average increase of +0.7 in comparison to simply encouraging the good actions. (5) Focusing only on good and bad tokens is sufficient, since setting Î³ to a non-zero value leads to a decrease of 1.9 on average. (6) The inferior performance of setting the weights as reward scores can be attributed to intrinsic inaccuracies of the reward scores, especially in out-of-distribution scenarios (Bai et al., 2022b). # 5 CONCLUSION In this paper, we have presented FIGA, a new approach that aligns language models with human preferences, by leveraging fine-grained quality signals to enhance the alignment quality during fine- tuning.\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "---\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: to the responses of corresponding quality, employs SFT on it and computes loss only for the specially masked response tokens. (4) RRHF (Yuan et al., 2023): it applies SFT on the optimal responses, and further optimizes the ranking loss among responses from multiple sources by encouraging the model to achieve a greater conditional log probability for the response that holds a superior ranking. IMPLEMENTATION DETAILS Training Datasets For our SPA dataset mentioned in Section 3.1, we broadly select the follow- ing datasets as our initial instances pool: HH-RLHF (Bai et al., 2022a), ShareGPT (ShareGPT, 2023), Synthetic Instruct GPT-J Pairwise (Dahoas, 2023), Stanford SHP (Ethayarajh et al., 2022), and OpenOrca (Lian et al., 2023). We employ the Alpaca-7b model Taori et al. (2023) as the rollout model for generating responses Ë Y , and gpt-3.5-turbo to revise and obtain Ë Y . The prompt used for revision can be found in Appendix A.2 As for the filtering process, we utilize OpenAssistant/reward-model-deberta-v3-large-v2 (OpenAssistant, 2023) as the reward model. According to the reward score distribution, we empirically set the threshold values Î·1 = 1, Î·2 = 3, Î·3 = 3.5, respectively. The statistics of reward scores and edit operations for the SPA dataset are presented in Table 1, while the distribution of the reward scores is illustrated in Figure 2. We can find that the initial response Ë Y has a large distribution gap with the reference distribution Y , which may cause the model hard to learn from the golden target. In contrast, our revised response is closer to the original distribution but with higher quality, making the rollout model easier to learn. The final SPA dataset we obtained consists of 17,333 instances. Model Details (1) For SFT, we set learning rate to 1e-5 and batch size to 128.\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "---\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: Thus, it canâ t fully capture the correct alignment behaviors even demonstrated by what are good and bad behaviors. In this work, we introduce FIGA, a novel method that aligns language models with human prefer- ences. The core idea is to contrast a low-quality initial response from a LLMâ s output with a cor- responding high-quality revised response by another powerful LLM (e.g., ChatGPT), so that LLMs can be noted with what are newly added (good actions) and what are removed or substituted (bad actions) from such a revision process. Such fine-grained quality signals can be more useful than the widely used response-level quality signal. It can instruct LLMs to emphasize the learning of good actions and penalize the bad actions in a single response. To implement our approach, we first cu- rate an alignment dataset called SPA that pairs an initial response with a revised response under the guidance of the ground-truth demonstrations. We mainly keep the queries that a LLM performs less well on, and perform strict filtering. Further, we design a new fine-tuning method that assigns spe- cific token-level weights to different parts (e.g., good or bad tokens). Our learning loss can directly impose fine-grained reward scores to guide the learning of LLMs for improved alignment. To the best of our knowledge, it is the first attempt that leverages fine-grained quality signals for improving the alignment of LLMs without RL. Our approach can make LLMs better understand what are good and bad behaviors beyond simple imitation. By conducting extensive experiments, we demonstrate that FIGA shows promising performance in aligning language models with human preferences: our approach outperform the initial supervised-finetuned model by notable 3.2 points and the strong PPO method by 1.8 points. # 2 RELATED WORK In this section, we review the related work in the two aspects, namely reinforcement learning from human feedback and alignment without reinforcement learning. Reinforcement learning from human feedback Large-scale pre-training empowers large lan- guage models (LLMs) to acquire extensive knowledge, underscoring their remarkable potential across diverse tasks (Brown et al., 2020; Kojima et al., 2022; Zhang et al., 2022; Chowdhery et al., 2022). Nonetheless, models exclusively focus on next token prediction in pre-training phrase, while do not consider human preferences.\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "---\n",
            "Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "Content: 7 Preprint. data. This implies responses of FIGA are more in sync with human preferences, making it an exem- plary alignment model. FIGA also scores the highest on the MMLU benchmark, which demonstrates capable task solving abilities of our method, not just limited to alignment. In summary, FIGAâ s su- perior performance on benchmarks confirms the efficacy of our designing. Moreover, we compare the quality of responses from FIGA and other baselines on the Vicuna and WizardLM benchmarks, specifically evaluating the relative merits of each response. The results of this comparative analysis are illustrated in Figure 3. Mmm FIGAWins * Tie FIGA Loses @mm FIGAWins M Tie FIGA Loses Alpaca 7B 9% Alpaca 7B 12% PPO (SPA) 9% PPO (SPA) 14% PPO (85K) 10% PPO (85K) 13% RRHF 8% RRHF 18% CoH 22% CoH 22% SFT 20% SFT 25% ChatGPT Ek 24% ChatGPT EU 33% 0% 25% 50% 75% 100% 0% 25% 50% 75% 100% Figure 3: Win rate of FIGA vs other baselines on Vicuna (left) and WizardLM (right). 4.3 FURTHER ANALYSIS 4.3.1 PERFORMANCE COMPARISON W.R.T. SUBPAR ALIGNMENT DATASET As mentioned in Section 3.1, the steps involved in constructing the SPA dataset includes: (1) collect existing datasets, encompassing the preference datasets and the typical SFT datasets, (2) filter the data based on reward scores, (3) revise the initial responses using LLM. To examine the effectiveness of each of them, we develop the following dataset variants on which to conduct our FIGA: Preference: we only use preference data to construct initial instances pool D, with 3,971 samples. â ¢ Instruction: we construct the initial instances pool D with typical SFT data that the reward model had not encountered during its training, also totaling 3,971 instances. w/o reward filtering: this variant excludes data filtering based on reward scores. â\n",
            "ArXiv ID: 2311.04072\n",
            "Related Papers: ['2309.00267']\n",
            "\n",
            "\n",
            "What is RAG (Retrieval-Augmented Generation)?\n",
            "RAG is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources ...\n",
            "https://aws.amazon.com/what-is/retrieval-augmented-generation/\n",
            "---\n",
            "What is Retrieval-Augmented Generation (RAG)?\n",
            "RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases)\n",
            "https://cloud.google.com/use-cases/retrieval-augmented-generation\n",
            "---\n",
            "Retrieval-augmented generation\n",
            "Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information.\n",
            "https://en.wikipedia.org/wiki/Retrieval-augmented_generation\n",
            "---\n",
            "What Is Retrieval-Augmented Generation aka RAG\n",
            "Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched ...\n",
            "https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\n",
            "\n",
            "CONCLUSION\n",
            "----------\n",
            "This concludes the research based on available data.\n",
            "\n",
            "SOURCES\n",
            "-------\n",
            "Various tools and APIs were used to gather this information.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}